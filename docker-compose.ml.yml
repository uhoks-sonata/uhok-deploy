ml-inference:
    build:
      context: ../uhok-ml-inference
      dockerfile: Dockerfile
    image: uhok-ml-inference:0.1.2
    environment:
      - HF_HOME=/models/hf_cache
      - PYTHONUNBUFFERED=1
    volumes:
      - ml_cache:/models/hf_cache
      # - /data/models:/models:ro   # 모델 가중치는 이미지에 넣지 말고 볼륨/마운트 권장
    expose:
      - "8001"
    healthcheck:
      test: ["CMD-SHELL", "curl -f http://localhost:8001/health || exit 1"]
      interval: 30s
      timeout: 10s
      start_period: 60s
      retries: 3
    networks: [app_net]
    restart: unless-stopped
    logging:
      driver: json-file
      options: { max-size: "50m", max-file: "3" }
    profiles: ["with-ml"]  # 선택적 실행을 위한 프로필

volumes:
  ml_cache:

networks:
  app_net:
    driver: bridge
